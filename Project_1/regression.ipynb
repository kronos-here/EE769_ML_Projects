{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "2a747369b91572b4ada4e32643e31bca",
          "grade": false,
          "grade_id": "Title",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "Q1l_Lgo6P0gp"
      },
      "source": [
        "#**EE769 Introduction to Machine Learning**\n",
        "\n",
        "#Assignment 1: Gradient Descent, Linear Regression, and Regularization\n",
        "\n",
        "\n",
        "**Template and Instructions**\n",
        "\n",
        "\n",
        "\n",
        "1. Up to two people can team up, but only one should submit, and both should understand the entire code.\n",
        "2. Every line of code should end in a comment explaining the line\n",
        "3. It is recommended to solve the assignment in Google Colab.\n",
        "Write your roll no.s separated by commas here: 213230011, 213350005\n",
        "4. Write your names here: Sayan Ray, Ashish Patel\n",
        "5. There are two parts to the assignment. In the Part 1, the code format has to be strictly followed to enable auto-grading. In the second part, you can be creative.\n",
        "6. **You can discuss with other groups or refer to the internet without being penalized, but you cannot copy their code and modify it. Write every line of code and comment on your own.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "cae69541658b406d495bcffe0b7c0932",
          "grade": false,
          "grade_id": "cell-aa1aa8c8181f8810",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "l7h8HK7wP0gs"
      },
      "source": [
        "#**Part 1 begins ...**\n",
        "**Instructions to be strictly followed:**\n",
        "\n",
        "1. Do not add any code cells or markdown cells until the end of this part. Especially, do not change the blocks that say \"TEST CASES, DO NOT CHANGE\"\n",
        "2. In all other cells only add code where it says \"CODE HERE\".\n",
        "3. If you encounter any raise NotImplementedError() calls you may comment them out.\n",
        "\n",
        "We cannot ensure correct grading if you change anything else, and you may be penalised for not following these instructions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "0ecbf8e96219ab11b7716eaae61cb7f9",
          "grade": false,
          "grade_id": "cell-1e940101f37c7af2",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "blsZJkt_P0gs"
      },
      "source": [
        "## Import Statements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "fe70877a859ee0a10e9d591778022f8a",
          "grade": false,
          "grade_id": "Import_Statements",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "MHpuNvqDP0gt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "#Importing the packages of numpy, pandas and matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "e36a214573fc40f26b45e72215d61375",
          "grade": false,
          "grade_id": "Normalize_Title",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "bE1CT6d_P0gu"
      },
      "source": [
        "## Normalize function\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We normalize by subtracting the mean from the data and dividing that with the standard deviation.\n",
        "Now the coloumn of the data matrix X are all same kind of data recorded in different times or situations, so finding the mean and normalizing has tot be done coloumnwise.\n"
      ],
      "metadata": {
        "id": "DabYWu3XnYY6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "42293dda356d746ac19e9b6d81106261",
          "grade": false,
          "grade_id": "Normalize_Function",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "34kMYUfQP0gu"
      },
      "outputs": [],
      "source": [
        "def Normalize(X): # Output should be a normalized data matrix of the same dimension\n",
        "    '''\n",
        "    Normalize all columns of X using mean and standard deviation\n",
        "    '''\n",
        "    # By using the mean() of numpy package and setting the axis to 0 we can calculate the coloumnwise mean.\n",
        "    # By using the std() of numpy package and setting the axis to 0 we are calculating the coloumnwise standard deviation.\n",
        "    # Now subtracting mean from the actual matrix X and dividing it with the standard deviation we are obtaining the normalized\n",
        "\n",
        "    norX = (X - np.mean(X,axis=0)) / (np.std(X,axis=0))\n",
        "    return norX\n",
        "\n",
        "    #raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "7797dd606a68f028c945acd08850d108",
          "grade": true,
          "grade_id": "Normalize_Test",
          "locked": true,
          "points": 5,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "XiuRSqphP0gu"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "TEST CASES, DO NOT CHANGE\n",
        "'''\n",
        "''' case 1 - 1 dimensional array'''\n",
        "#X=np.array([[1,2,3],[3,4,5],[7,8,9]])\n",
        "X1=np.array([1,2,3])\n",
        "np.testing.assert_array_almost_equal(Normalize(X1),np.array([-1.224,  0.      ,  1.224]),decimal=3)\n",
        "''' case 2 - 2 dimensional array'''\n",
        "X2=np.array([[4,7,6],[3,8,9],[5,11,10]])\n",
        "np.testing.assert_array_almost_equal(Normalize(X2),np.array([[ 0.  , -0.980581, -1.372813],[-1.224745, -0.392232,  0.392232],[ 1.224745,  1.372813,  0.980581]]))\n",
        "''' case 3 - 1 dimensional array with float'''\n",
        "X3=np.array([5.5,6.7,3.2,6.7])\n",
        "np.testing.assert_array_almost_equal(Normalize(X3),np.array([-0.017,  0.822, -1.627,  0.822]),decimal=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "63ee774a26a0438a0b2f5ec298eac80e",
          "grade": false,
          "grade_id": "cell-66ab98f84d3c58fa",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "Sxaf4UzwP0gv"
      },
      "source": [
        "## Prediction Function\n",
        "\n",
        "Given X and w, compute the predicted output. Do not forget to add 1's in X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "5976e32bd96bc4bdc2b2efd61e98441c",
          "grade": false,
          "grade_id": "Prediction",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "HuAWwUfsP0gv"
      },
      "outputs": [],
      "source": [
        "def Prediction (X, w): # Output should be a prediction vector y\n",
        "    '''\n",
        "    Compute Prediction given an input datamatrix X and weight vecor w. Output y = [X 1]w where 1 is a vector of all 1s\n",
        "    '''\n",
        "    # Generating the extra coloumns of 1's to take care of the bias terms in weight array and attaching it at te end of X (data) matrix.\n",
        "    p = np.ones(len(X))\n",
        "    X = np.c_[X,p]\n",
        "\n",
        "    # And thus by multiplying the data and weights we get our prediction.\n",
        "    y = np.dot(X,w)\n",
        "    return y\n",
        "\n",
        "    #raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "741f088432fb2ac3c49d3c9711d5c9c5",
          "grade": true,
          "grade_id": "PredictionTest",
          "locked": true,
          "points": 5,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "i2d8tBl9P0gw"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "TEST CASES, DO NOT CHANGE\n",
        "'''\n",
        "''' case 1 - Known input output matrix and weights 1'''\n",
        "X1 = np.array([[3,2],[1,1]])\n",
        "w1 = np.array([2,1,1])\n",
        "np.testing.assert_array_equal(Prediction(X1,w1),np.array([9,4]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "227f6e0da2f8c6d631599c44041e3b65",
          "grade": false,
          "grade_id": "cell-6fda2832d5967072",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "4H6nliXuP0gx"
      },
      "source": [
        "## Loss Functions\n",
        "\n",
        "Code the four  loss functions:\n",
        "\n",
        "1. MSE loss is only for the error\n",
        "2. MAE loss is only for the error\n",
        "3. L2 loss is for MSE and L2 regularization, and can call MSE loss\n",
        "4. L1 loss is for MSE and L1 regularization, and can call MSE loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MSE loss or Mean Square Error Loss, as the name suggests its the Mean of the sum of square of individual errors. That is:\n",
        "\n",
        "(1/n)*Σ(t-y)^2  \n",
        "\n",
        "(n being the total number of y or t terms)"
      ],
      "metadata": {
        "id": "O8gtI3Cjo9it"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "2c3e17b8559f0dfdfecdc8d242aba152",
          "grade": false,
          "grade_id": "MSE_Loss",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "ME3v_nUcP0gx"
      },
      "outputs": [],
      "source": [
        "def MSE_Loss (X, t, w, lamda =0): # Ouput should be a single number\n",
        "    '''\n",
        "    lamda=0 is a default argument to prevent errors if you pass lamda to a function that doesn't need it by mistake.\n",
        "    This allows us to call all loss functions with the same input format.\n",
        "\n",
        "    You are encouraged read about default arguments by yourself online if you're not familiar.\n",
        "    '''\n",
        "    # Firstly we need to generate the output that is predicted with the given Data and weights.\n",
        "    y = Prediction (X, w)\n",
        "\n",
        "    # Now simply we find the difference (t - y) and square it using the square() and their mean using mean()\n",
        "    MseLoss = np.mean(np.square(np.subtract(t,y)))\n",
        "    return MseLoss\n",
        "\n",
        "    #raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "923a1372a401b41d6ef6b0749a49ff66",
          "grade": true,
          "grade_id": "MSE_Loss_Test",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "jH_tAheDP0gx"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "TEST CASES, DO NOT CHANGE\n",
        "'''\n",
        "''' case 1 '''\n",
        "X=np.array([[3,6,5],[4.5,6.6,6]])\n",
        "t=np.array([4,5.5])\n",
        "w=np.array([2,-1,0.5,1])\n",
        "np.testing.assert_almost_equal(MSE_Loss(X,t,w),0.53,decimal=3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MAE loss or Mean Absoloute Error Loss, as the name suggests its the Mean of the sum of absoloute values of individual errors. That is:\n",
        "\n",
        "(1/n)*Σ|(t-y)|  \n",
        "\n",
        "(n being the total number of y or t terms)"
      ],
      "metadata": {
        "id": "dz-gzCKPqVZs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "b0b3b381c7a3ac5d7dfbb5df647c0d85",
          "grade": false,
          "grade_id": "MAE_Loss",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "mTCrJNwTP0gy"
      },
      "outputs": [],
      "source": [
        "def MAE_Loss (X, t, w, lamda = 0): # Output should be a single number\n",
        "\n",
        "    # Firstly we need to generate the output that is predicted with the given Data and weights.\n",
        "    y = Prediction (X, w)\n",
        "\n",
        "    # Now simply we find the difference (t - y) and compute its absoloute it using the abs() and their mean using mean()\n",
        "    MaeLoss = np.mean(np.abs(np.subtract(t,y)))\n",
        "    return MaeLoss\n",
        "\n",
        "    #raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "f11d5fde7220893a9809f2954d18c5e5",
          "grade": true,
          "grade_id": "MAE_Loss_Test",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "zWxGUTrgP0gy"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "TEST CASES, DO NOT CHANGE\n",
        "'''\n",
        "''' case 1 '''\n",
        "X=np.array([[3,6,5],[4.5,6.6,6]])\n",
        "t=np.array([4,5.5])\n",
        "w=np.array([2,-1,0.5,1])\n",
        "np.testing.assert_almost_equal(MAE_Loss(X,t,w),0.700,decimal=3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "L2 loss is Mean Square Error Loss + L2 regularization, so we compute the Mean of the sum of square of individual errors along with that we use an additional term λΣw^2 That is:\n",
        "\n",
        "(1/n)*Σ(t-y)^2 + λΣw^2  \n",
        "\n",
        "(n being the total number of y or t terms)"
      ],
      "metadata": {
        "id": "4pHJROaEqqCN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "a8309762ea56bb6b24bbbf4a19cb16bc",
          "grade": false,
          "grade_id": "L2_Loss",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "O9xkQEyBP0gy"
      },
      "outputs": [],
      "source": [
        "def L2_Loss (X, t, w, lamda): # Output should be a single number based on L2-norm (with sqrt)\n",
        "    ''' Need to specify what inputs are'''\n",
        "    # Firstly we need to find the MSE loss for the generated output using the given Data and weights.\n",
        "    MseLoss = MSE_Loss (X, t, w, lamda)\n",
        "\n",
        "    # Now a thing to keep in mind, we are to exclude the bias term to compute regulation so a new weights array is generated with the bias term removed.\n",
        "    w_ac = np.delete(w, -1)\n",
        "\n",
        "    # According to the formula computing the regulization\n",
        "    reg = lamda * np.sqrt(np.sum(np.square(w_ac)))\n",
        "\n",
        "    # Finally the L2 loss\n",
        "    L2loss = MseLoss + reg\n",
        "    return L2loss\n",
        "    #raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "a5a409056f6797cdf8cbfab61b138c37",
          "grade": true,
          "grade_id": "L2_Loss_Test",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "vApEKvQoP0gz"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "TEST CASES, DO NOT CHANGE\n",
        "'''\n",
        "''' case 1 '''\n",
        "X=np.array([[3,6,5],[4.5,6.6,6]])\n",
        "t=np.array([4,5.5])\n",
        "w=np.array([2,-1,0.5,1])\n",
        "np.testing.assert_almost_equal(L2_Loss(X,t,w,0.5),1.675,decimal=3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "L1 loss is Mean Square Error Loss + L1 regularization, so we compute the Mean of the sum of square of individual errors along with that we use an additional term λΣ|w| That is:\n",
        "\n",
        "(1/n)*Σ(t-y)^2 + λΣ|w|\n",
        "\n",
        "(n being the total number of y or t terms)"
      ],
      "metadata": {
        "id": "IIIzDT-btLNJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "1af69c123815524cdc71c72e09ece638",
          "grade": false,
          "grade_id": "L1_Loss",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "1KdihUmoP0gz"
      },
      "outputs": [],
      "source": [
        "def L1_Loss (X, t, w, lamda): # Output should be a single number\n",
        "\n",
        "    # Firstly we need to find the MSE loss for the generated output using the given Data and weights.\n",
        "    MseLoss = MSE_Loss (X, t, w, lamda)\n",
        "\n",
        "    # Excluding the bias term to compute regulation so a new weights array is generated with the bias term removed.\n",
        "    w_ac = np.delete(w, -1)\n",
        "\n",
        "    # According to the formula computing the regulization\n",
        "    reg = lamda * np.sum(np.abs(w_ac))\n",
        "\n",
        "    # Finally the L1 loss\n",
        "    L1loss = MseLoss + reg\n",
        "    return L1loss\n",
        "    #raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "40224dd69dd8e6499a67271c33701bf4",
          "grade": true,
          "grade_id": "L1_Loss_Test",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "38V9rKjDP0gz"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "TEST CASES, DO NOT CHANGE\n",
        "'''\n",
        "''' case 1 '''\n",
        "X=np.array([[3,6,5],[4.5,6.6,6]])\n",
        "t=np.array([4,5.5])\n",
        "w=np.array([2,-1,0.5,1])\n",
        "np.testing.assert_almost_equal(L1_Loss(X,t,w,0.5),2.280,decimal=3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NRMSE of Normalized Root Mean Square Error as the name suggests is the normalized RMSE. And to Normalize we are going to divide using the standard deviation of the target."
      ],
      "metadata": {
        "id": "NtNop1snt1xj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "855bd2ded492e567b20f1d9705d9a97a",
          "grade": false,
          "grade_id": "NRMSE_Metric",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "twm-fygSP0gz"
      },
      "outputs": [],
      "source": [
        "def NRMSE_Metric (X, t, w, lamda=0): # Output should be a single number. RMSE/std_dev(t)\n",
        "\n",
        "    # First we are computing the RMSE for the given Data weigths and lambda.\n",
        "    rmse = np.sqrt(MSE_Loss (X, t, w, lamda =0))\n",
        "\n",
        "    # The standard deviation of the elements of t as given in the formula beside the method definition.\n",
        "    stdev = np.std(t)\n",
        "\n",
        "    # Computing the NRMSE\n",
        "    nrmse = rmse / stdev\n",
        "    return nrmse\n",
        "\n",
        "    #raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "256795799ceb6dfa0b32ab75e8ef30c4",
          "grade": true,
          "grade_id": "NRMSE_Metric_Test",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "eDuTFs7UP0g0"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "TEST CASES, DO NOT CHANGE\n",
        "'''\n",
        "''' Test case 1 '''\n",
        "X=np.array([[3,6,5],[4.5,6.6,6]])\n",
        "t=np.array([4,5.5])\n",
        "w=np.array([2,-1,0.5,1])\n",
        "np.testing.assert_almost_equal(NRMSE_Metric(X,t,w,0.5),0.970,decimal=3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "cc5df30e29bbe17d4bc7aa89210eb5cf",
          "grade": false,
          "grade_id": "Gradient_Title",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "tB2won1aP0g0"
      },
      "source": [
        "## Gradient function\n",
        "Each Loss function will have its own gradient function:\n",
        "\n",
        "1. MSE gradient is only for the error\n",
        "2. MAE gradient is only for the error\n",
        "3. L2 gradient is for MSE and L2 regularization, and can call MSE gradient\n",
        "4. L1 gradient is for MSE and L1 regularization, and can call MSE gradient"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Gradient of a prediction model is obtained by differentiating the loss function w.r.t. the"
      ],
      "metadata": {
        "id": "WSXToZTFzfZI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "0190492ee216edce701f93e697572fc1",
          "grade": false,
          "grade_id": "MSE_Gradient",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "OyoE1IptP0g1"
      },
      "outputs": [],
      "source": [
        "def MSE_Gradient (X, t, w, lamda=0): # Output should have the same size as w\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "    # Now adding the extra columns of 1s in the Data matrix and using that and the weights prepare a prediction\n",
        "    p = np.ones(len(X))\n",
        "    X = np.c_[X,p]\n",
        "    y = np.dot(X,w)\n",
        "\n",
        "    # Computing the error array by subtracting the prediction and target\n",
        "    err = y - t\n",
        "\n",
        "    # Just to properly multipy the data Matrix we prepare a transpose matrix\n",
        "    Xt = np.transpose(X)\n",
        "\n",
        "    # Finally the Gradient\n",
        "    MseGrad = np.dot(Xt,err)\n",
        "    return MseGrad\n",
        "    #raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "d61466927592a8776a8bfa688472bad4",
          "grade": true,
          "grade_id": "MSE_Gradient_Test",
          "locked": true,
          "points": 3,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "S0BdLRuxP0g1"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "TEST CASES, DO NOT CHANGE\n",
        "'''\n",
        "''' case 1 '''\n",
        "X=np.array([[3,6,5],[4.5,6.6,6]])\n",
        "t=np.array([4,5.5])\n",
        "w=np.array([2,-1,0.5,1])\n",
        "np.testing.assert_array_almost_equal(MSE_Gradient(X,t,w),np.array([2.55, 2.94, 2.9 , 0.4 ]),decimal=3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "49cc4e37cafad52f41853b4b0e71f89f",
          "grade": false,
          "grade_id": "MAE_Gradient",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "ktuxiXBdP0g1"
      },
      "outputs": [],
      "source": [
        "def MAE_Gradient (X, t, w, lamda=0): # Output should have the same size as w\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "    # Adding the extra columns of 1s in the Data matrix and using that and the weights prepare a prediction\n",
        "    p = np.ones(len(X))\n",
        "    X = np.c_[X,p]\n",
        "    y = np.dot(X,w)\n",
        "\n",
        "    # Computing the absolute error array by subtracting the prediction and target and using abs()\n",
        "    err = y - t\n",
        "    absErr = np.abs(err)\n",
        "\n",
        "    # Now for our case we are not compensating 0.5 so we have to multiply it seperately\n",
        "    absMat = 0.5 * (err / absErr)\n",
        "\n",
        "    # Just to properly multipy the data Matrix we prepare a transpose matrix\n",
        "    Xt = np.transpose(X)\n",
        "\n",
        "    # Finally the Gradient\n",
        "    MaeGrad = np.dot(Xt,absMat)\n",
        "    return MaeGrad\n",
        "    #raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "e1d418bf5351112ae8716877f8bb6359",
          "grade": true,
          "grade_id": "MAE_Gradient_Test",
          "locked": true,
          "points": 3,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "zYjqXX2hP0g2"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "TEST CASES, DO NOT CHANGE\n",
        "'''\n",
        "''' case 1 '''\n",
        "X=np.array([[3,6,5],[4.5,6.6,6]])\n",
        "t=np.array([4,5.5])\n",
        "w=np.array([2,-1,0.5,1])\n",
        "np.testing.assert_array_almost_equal(MAE_Gradient(X,t,w),np.array([0.75,  0.3 ,  0.5 , 0.]),decimal=3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "595829c0dd17df55a03d346fac507b4a",
          "grade": false,
          "grade_id": "L2_Gradient",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "x85HY7nWP0g2"
      },
      "outputs": [],
      "source": [
        "def L2_Gradient (X, t, w, lamda): # Output should have the same size as w\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "    # We are simply calculating the MSE gradient by directly using the functions defined above\n",
        "    MseGrad = MSE_Gradient (X, t, w, lamda)\n",
        "\n",
        "    # Now this manuvre done here is to ensure that the bias term is not taken during regularization\n",
        "    # But this is only ok for L2 since each element is going to be divided by norm of w so no chance of dividing by 0\n",
        "    w[-1] = 0\n",
        "    norm_w = ( np.sum(np.square(w)) )**0.5\n",
        "\n",
        "    # Thus now calculating the regulation for the gradient\n",
        "    regu = lamda * (w/norm_w)\n",
        "\n",
        "    #finally the L2 gradient\n",
        "    L2Grad = MseGrad + regu\n",
        "    return (L2Grad)\n",
        "    #raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "f292b42c3389b24e369234a7210b87bb",
          "grade": true,
          "grade_id": "L2_Gradient_Test",
          "locked": true,
          "points": 3,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "6zQYsxVOP0g2"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "TEST CASES, DO NOT CHANGE\n",
        "'''\n",
        "''' case 1 '''\n",
        "X=np.array([[3,6,5],[4.5,6.6,6]])\n",
        "t=np.array([4,5.5])\n",
        "w=np.array([2,-1,0.5,1])\n",
        "np.testing.assert_array_almost_equal(L2_Gradient(X,t,w,0.5),np.array([2.986, 2.721, 3.009 , 0.4 ]),decimal=3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "141736f1feda5aa225e008d749f0015c",
          "grade": false,
          "grade_id": "L1_Gradient",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "WhayepEfP0g3"
      },
      "outputs": [],
      "source": [
        "def L1_Gradient (X, t, w, lamda): # Output should have the same size as w\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "    # Calculating the MSE gradient by directly using the functions defined before.\n",
        "    MseGrad = MSE_Gradient (X, t, w, lamda)\n",
        "\n",
        "    # According to the formula we have to divide each w with their absoloute value thus we comute those values.\n",
        "    # One thing to keep in mind is that these divisors cannot be 0, this has to be ensured during training.\n",
        "    abs_w = np.abs(w)\n",
        "\n",
        "    # Now we simply compute the regularization.\n",
        "    regu = lamda * (w/abs_w)\n",
        "\n",
        "    # Making the bias term as 0 so as to keep it out of computation.\n",
        "    regu[-1] = 0\n",
        "\n",
        "    # Computing the L1 gradient\n",
        "    L1Grad = MseGrad + regu\n",
        "    return (L1Grad)\n",
        "    #raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "9097b61320e470f429a9f46c7da0b219",
          "grade": true,
          "grade_id": "L1_Gradient_Test",
          "locked": true,
          "points": 3,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "436G5em3P0g3"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "TEST CASES, DO NOT CHANGE\n",
        "'''\n",
        "''' case 1 '''\n",
        "X=np.array([[3,6,5],[4.5,6.6,6]])\n",
        "t=np.array([4,5.5])\n",
        "w=np.array([2,-1,0.5,1])\n",
        "np.testing.assert_array_almost_equal(L1_Gradient(X,t,w,0.5),np.array([3.05, 2.44, 3.4 , 0.4 ]),decimal=3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "1980a2f831b5e1ddc9b510c22ef502c7",
          "grade": false,
          "grade_id": "Gradient_Desc_Title",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "FoCYmSoNP0g3"
      },
      "source": [
        "## Gradient Descent Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "78cdd5ffa4590c10e19281722ccd537f",
          "grade": false,
          "grade_id": "Gradient_Descent",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "Fyo64Wi8P0g3"
      },
      "outputs": [],
      "source": [
        "def Gradient_Descent (X, X_val, t, t_val, w, lamda, max_iter, epsilon, lr, lossfunc, gradfunc): # See output format in 'return' statement\n",
        "\n",
        "    # Now we simply try to adjust the value of w based on gradient descent with then given loss and gradient function.\n",
        "    # So my aim is to keep optimizing the value of w based on a fixed lamda on the training set until the change loss is less than epsilon.\n",
        "\n",
        "    #to compare the previous loss for an un-updated w and the present updated w in every iteration.\n",
        "    prev_cost = 0\n",
        "\n",
        "    # we keep iterating until we iterate max_iter times.\n",
        "    for i in range(max_iter):\n",
        "        curr_cost = lossfunc (X, t, w, lamda) #calculating the loss for a current w\n",
        "        if (abs(prev_cost - curr_cost)<=epsilon):\n",
        "            break # If the difference between consecutive losses are almost the same then we brak out\n",
        "\n",
        "        # If change in loss is still more then epsilon we update the weights using gradent descent\n",
        "        w = w - (lr * gradfunc(X, t, w, lamda))\n",
        "        prev_cost = curr_cost\n",
        "\n",
        "\n",
        "    # so now we have the best set of trainied weights we put them in to the w_final variable and compute the final training and vaildation losses\n",
        "    # Mind you these losses are are MSE not RMSE\n",
        "    w_final = w\n",
        "\n",
        "    # The training loss has to be computed on the training dataset and training target, based on a particular lambda\n",
        "    train_loss_final = lossfunc(X, t, w_final, lamda)\n",
        "\n",
        "    # The validation loss has to be computed on the validation dataset and validation target, based on a particular lambda\n",
        "    validation_loss_final = lossfunc(X_val, t_val, w_final, lamda)\n",
        "\n",
        "    # The validation NRMSE has to be computed on the validation dataset and Validation target using the NRMSE_Metric function, based on a particular lambda\n",
        "    validation_NRMSE = NRMSE_Metric (X_val, t_val, w_final, lamda)\n",
        "\n",
        "    #raise NotImplementedError()\n",
        "    return w_final, train_loss_final, validation_loss_final, validation_NRMSE #You should return variables structured like this.\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "15a67336bed20a51f33820bf615186e8",
          "grade": true,
          "grade_id": "Gradient_Check",
          "locked": true,
          "points": 20,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "GsDB7wJpP0g4"
      },
      "outputs": [],
      "source": [
        "\n",
        "'''\n",
        "TEST CASES, DO NOT CHANGE\n",
        "'''\n",
        "X=np.array([[23,24],[1,2]])\n",
        "t=np.array([4,5])\n",
        "X_val=np.array([[3,4],[5,6]])\n",
        "t_val=np.array([3,4])\n",
        "w=np.array([3,2,1])\n",
        "results =Gradient_Descent (X, X_val, t, t_val, w, 0.1, 100, 1e-10, 1e-5, L2_Loss,L2_Gradient)\n",
        "np.testing.assert_allclose([results[1]],[697.919],rtol =0.05)\n",
        "np.testing.assert_allclose([results[2]],[20],atol=5) # we expect around 17.5  but some students got 24 which we will also accept\n",
        "#Instructor Values of results[1] and results [2] are 697.919 and 17.512 respectively"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "24f327aa6a14590077f907ee8323724c",
          "grade": false,
          "grade_id": "PseudoInvTitle",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "zuQrcqBKP0g4"
      },
      "source": [
        "## Pseudo Inverse Method\n",
        "\n",
        "You have to implement a slightly more advanced version, with L2 penalty:\n",
        "\n",
        "w = (X' X + lambda I)^(-1) X' t.\n",
        "\n",
        "See, for example: Section 2 of https://web.mit.edu/zoya/www/linearRegression.pdf\n",
        "\n",
        "Here, the column of 1's in assumed to be included in X"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In this method I am using the given formula**:\n",
        "\n",
        "w = (X' X + lambda I)^(-1) X' t\n",
        "\n",
        "I am going to denote X' X as A\n",
        "\n",
        "And denoting: (X' X + lambda I) as B"
      ],
      "metadata": {
        "id": "afX5rL5d60-F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "928c9b105c4bcf3c132c7520a140d509",
          "grade": false,
          "grade_id": "PseudoInv",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "mI8koKQgP0g5"
      },
      "outputs": [],
      "source": [
        "def Pseudo_Inverse (X, t, lamda): # Output should be weight vector\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "    # Adding the coloumn of 1s for bias terms\n",
        "    p = np.ones(len(X))\n",
        "    X = np.c_[X,p]\n",
        "\n",
        "    # The transpose of the dataset\n",
        "    Xt = np.transpose(X)\n",
        "\n",
        "    # The X'X matrix\n",
        "    A = np.dot(Xt,X)\n",
        "\n",
        "    # To form the Identity matrix we need the dimension of X'X\n",
        "    n = len(A)\n",
        "    I = np.identity(n, dtype = float)\n",
        "\n",
        "    # Computing (X' X + lambda I)\n",
        "    B = np.linalg.inv(A + (lamda*I))\n",
        "\n",
        "    # Finally the weights array is this\n",
        "    w = np.dot(np.dot(B,Xt),t)\n",
        "    return (w)\n",
        "    #raise NotImplementedError() a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "a5178b8280650ed5d1d55dc6bbb38a29",
          "grade": true,
          "grade_id": "PseudoInvTest",
          "locked": true,
          "points": 10,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "etVVjyXNP0g5"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "TEST CASES, DO NOT CHANGE\n",
        "'''\n",
        "''' case 1 - other data'''\n",
        "X=np.array([[3,6,5],[4.5,6.6,6]])\n",
        "t=np.array([4,5.5])\n",
        "np.testing.assert_array_almost_equal(Pseudo_Inverse(X,t,0.5),np.array([ 0.491,  0.183,  0.319, -0.002]),decimal=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "8748fcc0dc96df6e7bb48a4e315927d3",
          "grade": false,
          "grade_id": "cell-e0fa02d7eecfb851",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "CPoO2NNeP0g5"
      },
      "source": [
        "#... Part 1 ends Below this you be more creative. Just comment out the lines where you save files (e.g. test predictions)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "c9047f75587bfee6b7b0a4b244efc7fa",
          "grade": false,
          "grade_id": "cell-10f0d275f5cd36f6",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "Zg9CdGdQP0g6"
      },
      "source": [
        "#**Part 2 begins ...**\n",
        "\n",
        "**Instructions to be loosely followed (except number 8):**\n",
        "\n",
        "1. Add more code and text cells between this and the last cell.\n",
        "2. Read training data from: https://www.ee.iitb.ac.in/~asethi/Dump/TempTrain.csv only. Do not use a local copy of the dataset.\n",
        "3. Find the best lamda for **MSE+lamda*L2(w)** loss function. Plot training and validation RMSE vs. 1/lamda (1/lamda represents model complexity). Print weights, validation RMSE, validation NRMSE for the best lamda.\n",
        "4. Find the best lamda for **MSE+lamda*L1(w)** loss function. Plot training and validation RMSE vs. 1/lamda (1/lamda represents model complexity). Print weights, validation RMSE, validation NRMSE for the best lamda.\n",
        "5. Find the best lamda for the **pseudo-inv method**. Plot training and validation RMSE vs. 1/lamda (1/lamda represents model complexity). Print weights, validation RMSE, validation NRMSE for the best lamda.\n",
        "6. Plot a predicted vs. actual plot for the validation data for the best model, and compute its R^2.\n",
        "7. Write your observations and conclusions.\n",
        "8. Read test data from: https://www.ee.iitb.ac.in/~asethi/Dump/TempTest.csv only. Do not use a local copy of the dataset. Predict its dependent (missing last column) using the model with the lowest MSE, RMSE, or NRMSE. Save it as a file RollNo1_RollNo2_1.csv.\n",
        "9. **Disable the prediction csv file saving statement and submit this entire .ipynb file, .py file, and .csv file as a single RollNo1_RollNo2_1.zip file.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before starting this I would highly advise the user to run the cells from top to bottom, as there are variable declarations that might affect the code if the oreder is not maintained. Thank You\n",
        "\n",
        "\n",
        "**The training set is in :** https://www.ee.iitb.ac.in/~asethi/Dump/TempTrain.csv"
      ],
      "metadata": {
        "id": "Q5tnBKzS8Sgd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math as mt\n",
        "# I am importing these again in case only this section is run and the one on top is not.\n",
        "# Opening this csv file required this and google colab did not have this so installing it seperately\n",
        "!pip install fsspec\n",
        "\n",
        "url_path = \"https://www.ee.iitb.ac.in/~asethi/Dump/TempTrain.csv\"\n",
        "df = pd.read_csv( url_path )\n",
        "\n",
        "#now that we have the data frame I am convertin it to a numpy matrix.\n",
        "arr = df.to_numpy()\n",
        "\n",
        "arr_norm = Normalize(arr) # normalizing, as the differences in the data might cause problems.\n",
        "np.random.shuffle(arr_norm) # shuffle.\n",
        "\n",
        "# splitting into Training and Validation sets.\n",
        "t_set = arr_norm[:,-1]\n",
        "X_set = arr_norm[:,0:-1]\n",
        "\n",
        "# Selecting 80% of the dataset as training data and 20% as validation data.\n",
        "\n",
        "# Validation data will be 20% so declaring that particular point.\n",
        "val_len = int(0.2 * len(t_set))\n",
        "\n",
        "# Taking 20% off the total data from the top (index 0 to 20% point) for valiaton purpose.\n",
        "t_val = t_set[0:val_len]\n",
        "X_val = X_set[0:val_len,:] # we are specifying the rows and taing all coloumns\n",
        "\n",
        "# Data from that 20% point till the end goes to model Training.\n",
        "t = t_set[val_len:len(t_set)]\n",
        "X = X_set[val_len:len(t_set),:] # we are specifying the rows and taing all coloumns\n",
        "\n",
        "# Now these variables will help us later on when we compare model superiority\n",
        "# The err_<model> variables store the NRMSE errors of each model\n",
        "err_L2 = 0\n",
        "err_L1 = 0\n",
        "err_PI = 0\n",
        "# The w_<model> variables store the weights for best lambda of each model\n",
        "w_L2 = []\n",
        "w_L1 = []\n",
        "w_PI = []\n",
        "\n",
        "# To store the best model parameters out of the three\n",
        "bestModel_w = []"
      ],
      "metadata": {
        "id": "Rph0bHWtixlL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5386592-4a65-4a4d-ac56-3c669f9400f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fsspec\n",
            "  Downloading fsspec-2022.7.1-py3-none-any.whl (141 kB)\n",
            "\u001b[K     |████████████████████████████████| 141 kB 4.9 MB/s \n",
            "\u001b[?25hInstalling collected packages: fsspec\n",
            "Successfully installed fsspec-2022.7.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finding the best lamda for **MSE+lamda*L2(w)** loss function i.e. L2 loss function.\n",
        "\n",
        "Plotting the training and validation RMSE vs. 1/lamda.\n",
        "\n",
        "Printing weights, validation RMSE, validation NRMSE for the best lamda."
      ],
      "metadata": {
        "id": "dvF4VUEE-mXD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bestLamL2 ():\n",
        "\n",
        "  # So that the global variables can be modified we need to declare them global.\n",
        "  global err_L2\n",
        "  global w_L2\n",
        "\n",
        "  # So we are to find a particular λ\n",
        "  # Before that we have to have a particular starting weights array it can be anything but has to be initialized.\n",
        "  sh = X.shape\n",
        "  w = np.random.randint(-5,5,sh[1])\n",
        "\n",
        "  # Adding that bias temrm by adding a coloumn of 1s\n",
        "  w = np.append(w,1)\n",
        "\n",
        "  # These lists are to plot the required graphs\n",
        "  lamInv = []  # Complexity\n",
        "  tr_loss = [] # Training loss\n",
        "  val_loss = []# Validation loss\n",
        "\n",
        "  # I am using the same values as used to verify the gradient descent function's correctness\n",
        "  epsilon = 1e-10\n",
        "  max_iter = 100\n",
        "  lr = 1e-5\n",
        "\n",
        "  # I have observed that generally the error changes very less for higher values of complexity\n",
        "  # Thus I am considering plotting for complexities in the range of 0.1 to 40 as it captures the required parts\n",
        "  lamda = 10.0\n",
        "\n",
        "  # I am running the values of lambda in reverse so that the curve is plotted in increasing order of complexity\n",
        "  while (lamda >= 0.025):\n",
        "\n",
        "    # The gradient descent method gives the losses directly so using that\n",
        "    resultsL2 = Gradient_Descent (X, X_val, t, t_val, w, lamda, max_iter, epsilon, lr, L2_Loss,L2_Gradient)\n",
        "\n",
        "    # Appending the losses to the respective lists\n",
        "    tr_loss = np.append(tr_loss,resultsL2[1])\n",
        "    val_loss = np.append(val_loss,resultsL2[2])\n",
        "    lamInv = np.append(lamInv,(1/lamda))\n",
        "    lamda = lamda - 0.001\n",
        "\n",
        "  # Converting all the MSE losses to RMSE losses\n",
        "  tr_loss = np.sqrt(tr_loss)\n",
        "  val_loss = np.sqrt(val_loss)\n",
        "\n",
        "  # Now where to stop as the lowest value is better and better for increasing complexity\n",
        "  # Thus if the value is within 5% of its decay we are selecting that value\n",
        "  min_lim = min(val_loss)\n",
        "  max_lim = max(val_loss)\n",
        "\n",
        "  # tolerance stores the value at which we get the best lambda\n",
        "  tolerance = 0.05* (max_lim - min_lim) + min_lim\n",
        "\n",
        "  # c serves as the element index in the respective lists, to locate the best lambda\n",
        "  c = 0\n",
        "\n",
        "  # So now we go through the validation losses and select the first lambda that indicates the 95% decay of the loss\n",
        "  for i in val_loss :\n",
        "    if (i <= tolerance):\n",
        "      break\n",
        "    c = c + 1\n",
        "\n",
        "  # So the best lambda is found out by inverting the complexiy (in lamInv list) at index c\n",
        "  best_lamda = 1/lamInv[c]\n",
        "\n",
        "  # Using the gradient descent function one last time to get the best set of weights\n",
        "  finalL2 = Gradient_Descent (X, X_val, t, t_val, w, best_lamda, max_iter, epsilon, lr, L2_Loss,L2_Gradient)\n",
        "  # Now displaying them\n",
        "  print (\"Best Lamda \", str(best_lamda))\n",
        "  print()\n",
        "  print (\"Weights \", str(finalL2[0]))\n",
        "  print()\n",
        "  print (\"Validation RMSE Loss \", str(val_loss[c]))\n",
        "  print()\n",
        "  print (\"Validation NRMSE Loss \", str(finalL2[3]))\n",
        "  print()\n",
        "\n",
        "  #storing the comparison parameters\n",
        "  err_L2 = finalL2[3]\n",
        "  w_L2 = finalL2[0]\n",
        "\n",
        "  # Plotting the graphs with a mark on the best lambda\n",
        "  plt.plot(lamInv[c], tr_loss[c], marker=\"o\", markersize=5, markeredgecolor=\"red\", markerfacecolor=\"red\",label=\"Best Lambda\")\n",
        "  plt.plot (lamInv,tr_loss)\n",
        "  plt.xlabel(\"1/λ\")\n",
        "  plt.ylabel(\"Training RMSE Loss\")\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "  plt.plot(lamInv[c], val_loss[c], marker=\"o\", markersize=5, markeredgecolor=\"red\", markerfacecolor=\"red\",label=\"Best Lambda\")\n",
        "  plt.plot (lamInv, val_loss)\n",
        "  plt.xlabel(\"1/λ\")\n",
        "  plt.ylabel(\"Validation RMSE Loss\")\n",
        "  plt.legend()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "LTlFF0HwOjkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this cell is just to run the entire method above and generate the Results\n",
        "bestLamL2 ()"
      ],
      "metadata": {
        "id": "xSgAnraZrWMy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 767
        },
        "outputId": "bacfa022-53ff-4edc-e65d-a58e96d65778"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Lamda  0.2530000000001028\n",
            "\n",
            "Weights  [-1.23880314e+00  2.15281916e+00 -1.50293033e+00 -5.60091154e-01\n",
            " -1.12160082e+00  4.78277919e-01 -4.27067506e-01  7.12889390e-01\n",
            " -1.18320271e+00  5.59539089e-01  1.08405912e+00 -8.81424375e-01\n",
            "  1.71300348e-01 -2.51236990e-02 -6.07349003e-02  7.60535656e-02\n",
            "  2.98409624e-01 -1.36707406e-02  1.15595519e+00 -8.54861713e-01\n",
            "  4.25020417e-01 -1.01042813e-04]\n",
            "\n",
            "Validation RMSE Loss  2.0704842783617274\n",
            "\n",
            "Validation NRMSE Loss  1.8193505091630224\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finding the best lamda for **MSE+lamda*L1(w)** loss function i.e. L1 loss function.\n",
        "\n",
        "Plotting the training and validation RMSE vs. 1/lamda.\n",
        "\n",
        "Printing weights, validation RMSE, validation NRMSE for the best lamda."
      ],
      "metadata": {
        "id": "01Jd3eNUYlYg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bestLamL1 ():\n",
        "\n",
        "  # So that the global variables can be modified we need to declare them global.\n",
        "  global err_L1\n",
        "  global w_L1\n",
        "\n",
        "  # So we are to find a particular λ\n",
        "  # Before that we have to have a particular starting weights array it can be anything but has to be initialized.\n",
        "  sh = X.shape\n",
        "  w = np.random.randint(1,5,sh[1]) # Notice I avoided a 0 from  being a weight as it will cause an error if present\n",
        "\n",
        "  # Adding that bias temrm by adding a coloumn of 1s\n",
        "  w = np.append(w,1)\n",
        "\n",
        "  # These lists are to plot the required graphs\n",
        "  lamInv = []  # Complexity\n",
        "  tr_loss = [] # Training loss\n",
        "  val_loss = []# Validation loss\n",
        "\n",
        "  # I am using the same values as used to verify the gradient descent function's correctness\n",
        "  epsilon = 1e-10\n",
        "  max_iter = 100\n",
        "  lr = 1e-5\n",
        "\n",
        "  # I have observed that generally the error changes very less for higher values of complexity\n",
        "  # Thus I am considering plotting for complexities in the range of 0.1 to 40 as it captures the required parts\n",
        "  lamda = 10.0\n",
        "\n",
        "  # I am running the values of lambda in reverse so that the curve is plotted in increasing order of complexity\n",
        "  while (lamda >= 0.025):\n",
        "\n",
        "    # The gradient descent method gives the losses directly so using that\n",
        "    resultsL2 = Gradient_Descent (X, X_val, t, t_val, w, lamda, max_iter, epsilon, lr, L1_Loss,L1_Gradient)\n",
        "\n",
        "    # Appending the losses to the respective lists\n",
        "    tr_loss = np.append(tr_loss,resultsL2[1])\n",
        "    val_loss = np.append(val_loss,resultsL2[2])\n",
        "    lamInv = np.append(lamInv,(1/lamda))\n",
        "    lamda = lamda - 0.001\n",
        "\n",
        "  # Converting all the MSE losses to RMSE losses\n",
        "  tr_loss = np.sqrt(tr_loss)\n",
        "  val_loss = np.sqrt(val_loss)\n",
        "\n",
        "  # Now where to stop as the lowest value is better and better for increasing complexity\n",
        "  # Thus if the value is within 5% of its decay we are selecting that value\n",
        "  min_lim = min(val_loss)\n",
        "  max_lim = max(val_loss)\n",
        "\n",
        "  # tolerance stores the value at which we get the best lambda\n",
        "  tolerance = 0.05* (max_lim - min_lim) + min_lim\n",
        "\n",
        "  # c serves as the element index in the respective lists, to locate the best lambda\n",
        "  c = 0\n",
        "\n",
        "  # So now we go through the validation losses and select the first lambda that indicates the 95% decay of the loss\n",
        "  for i in val_loss :\n",
        "    if (i <= tolerance):\n",
        "      break\n",
        "    c = c + 1\n",
        "\n",
        "  # So the best lambda is found out by inverting the complexiy (in lamInv list) at index c\n",
        "  best_lamda = 1/lamInv[c]\n",
        "\n",
        "  # Using the gradient descent function one last time to get the best set of weights\n",
        "  # Now displaying them\n",
        "  finalL1 = Gradient_Descent (X, X_val, t, t_val, w, best_lamda, max_iter, epsilon, lr, L1_Loss,L1_Gradient)\n",
        "  print (\"Best Lamda \", str(best_lamda))\n",
        "  print()\n",
        "  print (\"Weights \", str(finalL1[0]))\n",
        "  print()\n",
        "  print (\"Validation RMSE Loss \", str(val_loss[c]))\n",
        "  print()\n",
        "  print (\"Validation NRMSE Loss \", str(finalL1[3]))\n",
        "\n",
        "  #storing the comparison parameters\n",
        "  err_L1 = finalL1[3]\n",
        "  w_L1 = finalL1[0]\n",
        "\n",
        "  # Plotting the graphs with a mark on the best lambda\n",
        "  plt.plot(lamInv[c], tr_loss[c], marker=\"o\", markersize=5, markeredgecolor=\"red\", markerfacecolor=\"red\",label=\"Best Lambda\")\n",
        "  plt.plot (lamInv,tr_loss)\n",
        "  plt.xlabel(\"1/λ\")\n",
        "  plt.ylabel(\"Training RMSE Loss\")\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "  plt.plot(lamInv[c], val_loss[c], marker=\"o\", markersize=5, markeredgecolor=\"red\", markerfacecolor=\"red\",label=\"Best Lambda\")\n",
        "  plt.plot (lamInv, val_loss)\n",
        "  plt.xlabel(\"1/λ\")\n",
        "  plt.ylabel(\"Validation RMSE Loss\")\n",
        "  plt.legend()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "1G1sLJGgOkEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this cell is just to run the entire method above and generate the Results\n",
        "bestLamL1()"
      ],
      "metadata": {
        "id": "Ngw2QPGsaaMc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        },
        "outputId": "747b24ac-556f-41cd-d06c-e92d86d85e57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Lamda  0.1350000000001027\n",
            "\n",
            "Weights  [ 0.42270774  0.23347993  0.76646569 -0.34176138  1.03485313 -0.72970751\n",
            " -0.23398165  0.06077622 -0.33141927  0.49362434 -0.44928896 -0.14848703\n",
            "  0.06357888 -0.07608411  0.01946544  0.09994237 -0.04609733 -0.01133291\n",
            "  0.11760808 -0.08363946  0.10887909  0.00538544]\n",
            "\n",
            "Validation RMSE Loss  1.119941150726551\n",
            "\n",
            "Validation NRMSE Loss  0.6860323881007023\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finding the best lamda for **pseudo-inv method**.\n",
        "\n",
        "Plotting the training and validation RMSE vs. 1/lamda.\n",
        "\n",
        "Printing weights, validation RMSE, validation NRMSE for the best lamda.\n"
      ],
      "metadata": {
        "id": "1YqB2gXfYsdZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bestLamPI ():\n",
        "\n",
        "  # So that the global variables can be modified we need to declare them global.\n",
        "  global err_PI\n",
        "  global w_PI\n",
        "\n",
        "  # So we are to find a particular λ\n",
        "  # Before that we have to have a particular starting weights array it can be anything but has to be initialized.\n",
        "  sh = X.shape\n",
        "  w = np.random.randint(-5,5,sh[1])\n",
        "\n",
        "  # Adding that bias temrm by adding a coloumn of 1s\n",
        "  w = np.append(w,1)\n",
        "\n",
        "  # These lists are to plot the required graphs\n",
        "  lamInv = []  # Complexity\n",
        "  tr_loss = [] # Training loss\n",
        "  val_loss = []# Validation loss\n",
        "\n",
        "  # I am using the same values as used to verify the gradient descent function's correctness\n",
        "  epsilon = 1e-10\n",
        "  max_iter = 100\n",
        "  lr = 1e-5\n",
        "\n",
        "  # Here too its observed that generally the error changes very less for higher values of complexity\n",
        "  # Thus I am considering plotting for complexities in the range of 0.002 to 10 as it captures the required part\n",
        "  lamda = 500.0\n",
        "\n",
        "  # I am running the values of lambda in reverse so that the curve is plotted in increasing order of complexity\n",
        "  while (lamda >= 1.0):\n",
        "    # Now in pseudo inverse method we dont need the gradient and related stuff.\n",
        "    # We can find the w by usig Pseudo Inverse function.\n",
        "    w_new = Pseudo_Inverse (X, t, lamda)\n",
        "\n",
        "    # Then using that we calculate the losses (MSE)\n",
        "    tr_loss = np.append(tr_loss, MSE_Loss (X, t, w_new, lamda))\n",
        "    val_loss = np.append(val_loss, MSE_Loss (X_val, t_val, w_new, lamda))\n",
        "    lamInv = np.append(lamInv,(1/lamda))\n",
        "    lamda = lamda - 0.01\n",
        "\n",
        "  # Converting all the MSE losses to RMSE losses\n",
        "  tr_loss = np.sqrt(tr_loss)\n",
        "  val_loss = np.sqrt(val_loss)\n",
        "\n",
        "  # In this method I am taking one precaution, sometimes the validation graph ight take a v shape\n",
        "  # So for that I am keeping an extra condition of stoppage from the training curve as well\n",
        "  # Now where to stop as the lowest value is better and better for increasing complexity\n",
        "  # Thus if the value is within 3% of its decay we are selecting that value\n",
        "  min_lim1 = min(val_loss)\n",
        "  max_lim1 = max(val_loss)\n",
        "\n",
        "  min_lim2 = min(tr_loss)\n",
        "  max_lim2 = max(tr_loss)\n",
        "\n",
        "  # tolerance stores the value at which we get the best lambda\n",
        "  tolerance1 = 0.03* (max_lim1 - min_lim1) + min_lim1\n",
        "  tolerance2 = 0.03* (max_lim2 - min_lim2) + min_lim2\n",
        "\n",
        "  # c serves as the element index in the respective lists, to locate the best lambda\n",
        "  c = 0\n",
        "\n",
        "  # So now we go through the validation losses and select the first lambda that indicates the 95% decay of the loss\n",
        "  while c <= len(tr_loss) :\n",
        "    if (val_loss[c] <= tolerance1)and(tr_loss[c] <= tolerance2):\n",
        "      break\n",
        "    c = c + 1\n",
        "\n",
        "  # So the best lambda is found out by inverting the complexiy (in lamInv list) at index c\n",
        "  best_lamda = 1/lamInv[c]\n",
        "\n",
        "  # Getting the best w corrosponding to the best value of lambda\n",
        "  w_new = Pseudo_Inverse (X, t, best_lamda)\n",
        "\n",
        "  #storing the comparison parameters\n",
        "  err_PI = NRMSE_Metric (X_val, t_val, w_new, best_lamda)\n",
        "  w_PI = w_new\n",
        "\n",
        "  # Thus all that's left is to display the values\n",
        "  print (\"Best Lamda \", str(best_lamda))\n",
        "  print()\n",
        "  print (\"Weights \", str(w_new))\n",
        "  print()\n",
        "  print (\"Validation RMSE Loss \", str(mt.sqrt(MSE_Loss (X_val, t_val, w_new, best_lamda))))\n",
        "  print()\n",
        "  print (\"Validation NRMSE Loss \", str(err_PI))\n",
        "  print()\n",
        "\n",
        "\n",
        "  #plotting the results with markers on the best lambda\n",
        "  plt.plot(lamInv[c], tr_loss[c], marker=\"o\", markersize=5, markeredgecolor=\"red\", markerfacecolor=\"red\", label =\"Best λ\")\n",
        "  plt.plot (lamInv,tr_loss)\n",
        "  plt.xlabel(\"1/λ\")\n",
        "  plt.ylabel(\"Training RMSE Loss\")\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "  plt.plot(lamInv[c], val_loss[c], marker=\"o\", markersize=5, markeredgecolor=\"red\", markerfacecolor=\"red\", label =\"Best λ\" )\n",
        "  plt.plot (lamInv, val_loss)\n",
        "  plt.xlabel(\"1/λ\")\n",
        "  plt.ylabel(\"Validation RMSE Loss\")\n",
        "  plt.legend()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "dnJL8dUSDczM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this cell is just to run the entire method above and generate the Results\n",
        "bestLamPI()"
      ],
      "metadata": {
        "id": "Ty9pgfKBOkjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion**\n",
        "\n",
        "Time taken by the L2 method of regression method is: ~ 14 - 15 min\n",
        "\n",
        "Time taken by the L1 method of regression method is: ~ 13 - 14 min\n",
        "\n",
        "Time taken by the Pseudo Inverse method is: ~ 1 - 2 min\n",
        "\n",
        "The plots of L2 loss and L1 loss model have a good learning rate whereas the Pseudo Inverse model for the same learning rate settles rapidly. But, it is ok-ish and the metrics perform good enough give values close to predicted (but not matching, proving no overfitting).  "
      ],
      "metadata": {
        "id": "Sh9o2iuOLjEn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print (\"The best model thus is :\")\n",
        "print (\"The MSE+lamda*L2(w) based model's NRMSE is :\",str(err_L2))\n",
        "print (\"The MSE+lamda*L2(w) based model's NRMSE is :\",str(err_L1))\n",
        "print (\"The MSE+lamda*L2(w) based model's NRMSE is :\",str(err_PI))\n",
        "print ()\n",
        "# So simply comparing the NRMSE errors and selecting the one with the lowest value to select the best model\n",
        "if (err_L2 <= err_L1) and (err_L2 <= err_PI):\n",
        "   print (\"MSE+lamda*L2(w) Gradient Descent is the Best Model\")\n",
        "   bestModel_w = w_L2\n",
        "elif (err_L1 <= err_L2) and (err_L1 <= err_PI):\n",
        "   print (\"MSE+lamda*L1(w) Gradient Descent is the Best Model\")\n",
        "   bestModel_w = w_L1\n",
        "else:\n",
        "   print (\"Pseudo Inverse method is the Best Model\")\n",
        "   bestModel_w = w_PI"
      ],
      "metadata": {
        "id": "bkwHVO2Nz5BP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotting a Line plot for the best model of prediction. Which in my case is for the case of Pseudo Inverse model. Its the fastest of the three and produces the lowest error for a give dataset.\n",
        "\n",
        "I will be taking the best lambda and the weights matrix directly and use it to produce a line plot to compare how good is the prediction in comparison to the actual value."
      ],
      "metadata": {
        "id": "y-Os2hmxXqVB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_y = Prediction(X_val, bestModel_w)\n",
        "\n",
        "# Now calulating the R^2 value of best model\n",
        "mean_t = np.mean(t_val)\n",
        "\n",
        "# The R^2 is given by ( Σ(Ypred - Ymean)^2 / Σ(Yactual - Ymean)^2 )\n",
        "numerator = np.sum(np.square(predicted_y - mean_t)) #The numerator\n",
        "denominator = np.sum(np.square(t_val - mean_t))     #The denominator\n",
        "R_squared = numerator/denominator\n",
        "print (\"R^2 parameter of the best model is: \"+str(R_squared))\n",
        "print ()\n",
        "\n",
        "# We are comparing the actual and predicted on the validation set using a line graph for 15 random data outputs\n",
        "print (\"The Actual vs predicted Model for a random 15 sets of inputs are:\")\n",
        "x_coor = [*range(1, 16, 1)]      # * helps us extract the elements of the range into an array\n",
        "pred_y_plot = predicted_y [1:16] # 15 values of Prediction\n",
        "val_y_plot  = t_val[1:16]        # The corrosponding 15 values actually\n",
        "\n",
        "plt.plot(x_coor, pred_y_plot, color='red',label=\"Prediction value\")\n",
        "plt.plot(x_coor, val_y_plot, color='blue',label=\"Actual value\")\n",
        "plt.xlabel(\"Sample Number\")\n",
        "plt.ylabel(\"Normalized Value of the quantity\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5nYoUUojXp4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are going to predict an output based on the dataset in the link: https://www.ee.iitb.ac.in/~asethi/Dump/TempTest.csv\n",
        "\n",
        "Using the methods which has performed the best."
      ],
      "metadata": {
        "id": "KdUK33kFXsII"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url_path2 =  \"https://www.ee.iitb.ac.in/~asethi/Dump/TempTest.csv\"\n",
        "df2 = pd.read_csv( url_path2 ) # reading data into a dataframe\n",
        "X_test = df2.to_numpy()        # converting the data into a numpy matrix\n",
        "prediction = Prediction(X_test, bestModel_w) #Using the best model weights from above to predict the values\n",
        "\n",
        "# Converting the array into dataframe\n",
        "df_out = pd.DataFrame(prediction)\n",
        "\n",
        "# save the dataframe as a csv file\n",
        "#df_out.to_csv(\"213230011_213350005_1.csv\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "93BkXK-pLiGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "de7f27538e4ad9db84b046b2752843f1",
          "grade": false,
          "grade_id": "cell-8a98f665f9567cec",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "cIKk0iDDP0g6"
      },
      "source": [
        "#**... Part 2 ends.**\n",
        "\n",
        "1. Write the name or roll no.s of friends from outside your group with whom you discussed the assignment here (no penalty for mere discussion without copying code):\n",
        "\n",
        "- I discussed a few areas of doubts with a friend Yashwant Nandi : 21335T002\n",
        "- I would like to thank the TAs who guided me.\n",
        "\n",
        "2. Write the links of sources on the internet referred here (no penalty for mere consultation without copying code):\n",
        "\n",
        "I reffered to the net mainly for the concepts and formulas but coding part is mostly my part, since I experimented with things like the tolerance for final or most preffered lambda value and so on, most of the code was according to the instructions and referrring to the package defined functions. Here are some of the websites to which I reffered to get formulas and understand algorithms.\n",
        "- https://www.geeksforgeeks.org/\n",
        "- https://www.w3schools.com/\n",
        "- https://stackoverflow.com/\n",
        "- https://numpy.org/doc/stable/index.html"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "blsZJkt_P0gs",
        "bE1CT6d_P0gu",
        "Sxaf4UzwP0gv",
        "4H6nliXuP0gx",
        "tB2won1aP0g0",
        "zuQrcqBKP0g4"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}